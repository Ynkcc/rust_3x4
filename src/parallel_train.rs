// parallel_train.rs - å¹¶è¡Œè‡ªå¯¹å¼ˆè®­ç»ƒç³»ç»Ÿ
//
// æ¶æ„è®¾è®¡:
// - ä¸»çº¿ç¨‹: è¿è¡Œæ¨¡å‹æ¨ç†æœåŠ¡ (InferenceServer)
// - å·¥ä½œçº¿ç¨‹æ± : æ¯ä¸ªçº¿ç¨‹è¿è¡Œç‹¬ç«‹çš„è‡ªå¯¹å¼ˆæ¸¸æˆ
// - é€šä¿¡: é€šè¿‡ channel å‘é€æ¨ç†è¯·æ±‚å’Œæ¥æ”¶ç»“æœ
// - æ‰¹é‡æ¨ç†: æ”¶é›†å¤šä¸ªè¯·æ±‚åæ‰¹é‡å¤„ç†ï¼Œæé«˜GPUåˆ©ç”¨ç‡

use banqi_3x4::game_env::{DarkChessEnv, Observation};
use banqi_3x4::mcts::{Evaluator, MCTS, MCTSConfig};
use banqi_3x4::nn_model::BanqiNet;
use anyhow::Result;
use std::sync::{Arc, mpsc};
use std::thread;
use std::time::{Duration, Instant};
use tch::{nn, nn::OptimizerConfig, Device, Tensor, Kind};
use rusqlite::{Connection, params};
use std::fs::OpenOptions;
use std::io::Write;

// ================ CSVæ—¥å¿—è®°å½• ================

/// è®­ç»ƒæ—¥å¿—è®°å½•ç»“æ„
#[derive(Debug, Clone)]
struct TrainingLog {
    iteration: usize,
    // æŸå¤±æŒ‡æ ‡ï¼ˆepochå¹³å‡ï¼‰
    avg_total_loss: f64,
    avg_policy_loss: f64,
    avg_value_loss: f64,
    policy_loss_weight: f64,
    value_loss_weight: f64,
    
    // åœºæ™¯1: R_A vs B_A
    scenario1_value: f32,
    scenario1_unmasked_a38: f32,
    scenario1_unmasked_a39: f32,
    scenario1_unmasked_a40: f32,
    scenario1_masked_a38: f32,
    scenario1_masked_a39: f32,
    scenario1_masked_a40: f32,
    
    // åœºæ™¯2: Hidden Threat
    scenario2_value: f32,
    scenario2_unmasked_a3: f32,
    scenario2_unmasked_a5: f32,
    scenario2_masked_a3: f32,
    scenario2_masked_a5: f32,
    
    // æ ·æœ¬ç»Ÿè®¡
    new_samples_count: usize,
    replay_buffer_size: usize,
    avg_game_steps: f32,
    red_win_ratio: f32,
    draw_ratio: f32,
    black_win_ratio: f32,
    avg_policy_entropy: f32,
    high_confidence_ratio: f32,
}

impl TrainingLog {
    fn write_header(csv_path: &str) -> Result<()> {
        let mut file = OpenOptions::new()
            .write(true)
            .create(true)
            .truncate(false)
            .open(csv_path)?;
        
        // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºï¼ˆæ–°æ–‡ä»¶éœ€è¦å†™å…¥è¡¨å¤´ï¼‰
        let metadata = std::fs::metadata(csv_path)?;
        if metadata.len() == 0 {
            writeln!(file, "iteration,avg_total_loss,avg_policy_loss,avg_value_loss,policy_loss_weight,value_loss_weight,\
                scenario1_value,scenario1_unmasked_a38,scenario1_unmasked_a39,scenario1_unmasked_a40,\
                scenario1_masked_a38,scenario1_masked_a39,scenario1_masked_a40,\
                scenario2_value,scenario2_unmasked_a3,scenario2_unmasked_a5,scenario2_masked_a3,scenario2_masked_a5,\
                new_samples_count,replay_buffer_size,avg_game_steps,red_win_ratio,draw_ratio,black_win_ratio,\
                avg_policy_entropy,high_confidence_ratio")?;
        }
        
        Ok(())
    }
    
    fn append_to_csv(&self, csv_path: &str) -> Result<()> {
        let mut file = OpenOptions::new()
            .write(true)
            .create(true)
            .append(true)
            .open(csv_path)?;
        
        writeln!(file, "{},{:.6},{:.6},{:.6},{:.3},{:.3},\
            {:.4},{:.4},{:.4},{:.4},{:.4},{:.4},{:.4},\
            {:.4},{:.4},{:.4},{:.4},{:.4},\
            {},{},{:.2},{:.4},{:.4},{:.4},{:.4},{:.4}",
            self.iteration,
            self.avg_total_loss, self.avg_policy_loss, self.avg_value_loss,
            self.policy_loss_weight, self.value_loss_weight,
            self.scenario1_value, self.scenario1_unmasked_a38, self.scenario1_unmasked_a39, self.scenario1_unmasked_a40,
            self.scenario1_masked_a38, self.scenario1_masked_a39, self.scenario1_masked_a40,
            self.scenario2_value, self.scenario2_unmasked_a3, self.scenario2_unmasked_a5,
            self.scenario2_masked_a3, self.scenario2_masked_a5,
            self.new_samples_count, self.replay_buffer_size, self.avg_game_steps,
            self.red_win_ratio, self.draw_ratio, self.black_win_ratio,
            self.avg_policy_entropy, self.high_confidence_ratio
        )?;
        
        Ok(())
    }
}

// ================ æ¨ç†è¯·æ±‚å’Œå“åº” ================

/// æ¨ç†è¯·æ±‚
#[derive(Debug)]
pub struct InferenceRequest {
    pub observation: Observation,
    pub action_masks: Vec<i32>,
    pub response_tx: mpsc::Sender<InferenceResponse>, // æ¯ä¸ªè¯·æ±‚æºå¸¦è‡ªå·±çš„å“åº”é€šé“
}

/// æ¨ç†å“åº”
#[derive(Debug, Clone)]
pub struct InferenceResponse {
    pub policy: Vec<f32>,
    pub value: f32,
}

// ================ æ‰¹é‡æ¨ç†æœåŠ¡å™¨ ================

pub struct InferenceServer {
    vs: nn::VarStore,     // æŒæœ‰ VarStoreï¼ˆåŒ…å«æ¨¡å‹æƒé‡ï¼‰
    net: BanqiNet,        // ç½‘ç»œç»“æ„
    device: Device,
    request_rx: mpsc::Receiver<InferenceRequest>,
    batch_size: usize,
    batch_timeout_ms: u64,
}

impl InferenceServer {
    pub fn new(
        model_path: &str,
        device: Device,
        request_rx: mpsc::Receiver<InferenceRequest>,
        batch_size: usize,
        batch_timeout_ms: u64,
    ) -> Result<Self> {
        let mut vs = nn::VarStore::new(device);
        let net = BanqiNet::new(&vs.root());
        
        // åŠ è½½æ¨¡å‹æƒé‡
        vs.load(model_path)?;
        
        Ok(Self {
            vs,
            net,
            device,
            request_rx,
            batch_size,
            batch_timeout_ms,
        })
    }

    /// è¿è¡Œæ¨ç†æœåŠ¡ï¼ˆé˜»å¡ï¼‰
    pub fn run(&self) {
        println!("[InferenceServer] å¯åŠ¨ï¼Œbatch_size={}, timeout={}ms", 
            self.batch_size, self.batch_timeout_ms);
        
        let mut batch = Vec::new();
        let mut total_requests = 0;
        let mut total_batches = 0;
        let batch_timeout = Duration::from_millis(self.batch_timeout_ms);
        
        loop {
            // å°è¯•å¿«é€Ÿæ”¶é›†ä¸€æ‰¹è¯·æ±‚
            
            // é¦–å…ˆå°è¯•éé˜»å¡æ¥æ”¶ï¼Œå¿«é€Ÿæ”¶é›†å¯ç”¨çš„è¯·æ±‚
            loop {
                match self.request_rx.try_recv() {
                    Ok(req) => {
                        batch.push(req);
                        total_requests += 1;
                        
                        // å¦‚æœè¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œç«‹å³å¤„ç†
                        if batch.len() >= self.batch_size {
                            break;
                        }
                    },
                    Err(mpsc::TryRecvError::Empty) => {
                        // æ²¡æœ‰æ›´å¤šè¯·æ±‚äº†
                        break;
                    },
                    Err(mpsc::TryRecvError::Disconnected) => {
                        // æ‰€æœ‰å‘é€è€…å·²æ–­å¼€
                        if !batch.is_empty() {
                            println!("[InferenceServer] æœ€ç»ˆæ‰¹æ¬¡: {} ä¸ªè¯·æ±‚", batch.len());
                            self.process_batch(&batch);
                            total_batches += 1;
                        }
                        println!("[InferenceServer] æ‰€æœ‰å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œé€€å‡º (æ€»è®¡: {} è¯·æ±‚, {} æ‰¹æ¬¡)", 
                            total_requests, total_batches);
                        return;
                    }
                }
            }
            
            // å¦‚æœæ”¶é›†åˆ°äº†è¯·æ±‚ï¼Œç«‹å³å¤„ç†ï¼ˆä¸ç­‰å¾…è¶…æ—¶ï¼‰
            if !batch.is_empty() {
                if total_batches % 4000 == 0 {
                    println!("[InferenceServer] å¤„ç†æ‰¹æ¬¡#{}: {} ä¸ªè¯·æ±‚", total_batches + 1, batch.len());
                }
                self.process_batch(&batch);
                total_batches += 1;
                batch.clear();
                continue;
            }
            
            // å¦‚æœæ²¡æœ‰è¯·æ±‚ï¼Œé˜»å¡ç­‰å¾…æ–°è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰
            match self.request_rx.recv_timeout(batch_timeout) {
                Ok(req) => {
                    batch.push(req);
                    total_requests += 1;
                },
                Err(mpsc::RecvTimeoutError::Timeout) => {
                    // è¶…æ—¶ä½†æ²¡æœ‰è¯·æ±‚ï¼Œç»§ç»­ç­‰å¾…
                    continue;
                },
                Err(mpsc::RecvTimeoutError::Disconnected) => {
                    println!("[InferenceServer] æ‰€æœ‰å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œé€€å‡º (æ€»è®¡: {} è¯·æ±‚, {} æ‰¹æ¬¡)", 
                        total_requests, total_batches);
                    return;
                }
            }
        }
    }

    /// æ‰¹é‡å¤„ç†æ¨ç†è¯·æ±‚
    fn process_batch(&self, batch: &Vec<InferenceRequest>) {
        if batch.is_empty() { return; }
        
        // let start_time = Instant::now();
        let batch_len = batch.len();
        
        // å‡†å¤‡æ‰¹é‡è¾“å…¥å¼ é‡
        let mut board_data = Vec::new();
        let mut scalar_data = Vec::new();
        let mut mask_data = Vec::new();
        
        for req in batch {
            // Board: [STATE_STACK_SIZE, 8, 3, 4] -> flatten
            let board_flat: Vec<f32> = req.observation.board.as_slice().unwrap().to_vec();
            board_data.extend_from_slice(&board_flat);
            
            // Scalars: [STATE_STACK_SIZE * 56]
            let scalars_flat: Vec<f32> = req.observation.scalars.as_slice().unwrap().to_vec();
            scalar_data.extend_from_slice(&scalars_flat);
            
            // Masks: [46]
            let masks_f32: Vec<f32> = req.action_masks.iter().map(|&m| m as f32).collect();
            mask_data.extend_from_slice(&masks_f32);
        }
        
        // æ„å»ºå¼ é‡: [batch, C, H, W]
        let board_tensor = Tensor::from_slice(&board_data)
            .view([batch_len as i64, 8, 3, 4])  // ç¦ç”¨çŠ¶æ€å †å å: STATE_STACK_SIZE=1, æ‰€ä»¥æ˜¯8é€šé“
            .to(self.device);
        
        let scalar_tensor = Tensor::from_slice(&scalar_data)
            .view([batch_len as i64, 56])  // ç¦ç”¨çŠ¶æ€å †å å: 56ä¸ªç‰¹å¾
            .to(self.device);
        
        let mask_tensor = Tensor::from_slice(&mask_data)
            .view([batch_len as i64, 46])
            .to(self.device);
        
        // å‰å‘æ¨ç†
        let (logits, values) = tch::no_grad(|| {
            self.net.forward(&board_tensor, &scalar_tensor)
        });
        
        // åº”ç”¨æ©ç å¹¶è®¡ç®—æ¦‚ç‡
        let masked_logits = &logits + (&mask_tensor - 1.0) * 1e9;
        let probs = masked_logits.softmax(-1, Kind::Float);
        
        // æå–ç»“æœå¹¶å‘é€å“åº”åˆ°å„è‡ªçš„é€šé“
        for (i, req) in batch.iter().enumerate() {
            let policy_slice = probs.get(i as i64);
            let mut policy = vec![0.0f32; 46];
            policy_slice.to_device(Device::Cpu).copy_data(&mut policy, 46);
            
            let value = values.get(i as i64).squeeze().double_value(&[]) as f32;
            
            let response = InferenceResponse {
                policy,
                value,
            };
            
            // å‘é€å“åº”åˆ°è¯·æ±‚è€…çš„ä¸“å±é€šé“ï¼ˆå¿½ç•¥å‘é€å¤±è´¥ï¼‰
            let _ = req.response_tx.send(response);
        }
        
        
        // let elapsed = start_time.elapsed();
        // if batch_len >= 4 {  // åªåœ¨æ‰¹é‡è¾ƒå¤§æ—¶è¾“å‡ºæ—¥å¿—
        //     println!("[InferenceServer] æ‰¹æ¬¡å¤„ç†: {} ä¸ªè¯·æ±‚è€—æ—¶ {:.2}ms", 
        //         batch_len, elapsed.as_secs_f64() * 1000.0);
        // }
    }
}

// ================ Channel Evaluatorï¼ˆç”¨äºMCTSï¼‰ ================

pub struct ChannelEvaluator {
    request_tx: mpsc::Sender<InferenceRequest>,
}

impl ChannelEvaluator {
    pub fn new(request_tx: mpsc::Sender<InferenceRequest>) -> Self {
        Self { request_tx }
    }
}

impl Evaluator for ChannelEvaluator {
    fn evaluate(&self, env: &DarkChessEnv) -> (Vec<f32>, f32) {
        // ä¸ºæ­¤æ¬¡è¯·æ±‚åˆ›å»ºä¸€æ¬¡æ€§å“åº”é€šé“
        let (response_tx, response_rx) = mpsc::channel();
        
        // å‘é€æ¨ç†è¯·æ±‚
        let req = InferenceRequest {
            observation: env.get_state(),
            action_masks: env.action_masks(),
            response_tx,
        };
        
        self.request_tx.send(req).expect("æ¨ç†æœåŠ¡å·²æ–­å¼€");
        
        // ç­‰å¾…å“åº”ï¼ˆé˜»å¡ï¼‰
        let resp = response_rx.recv().expect("æ¨ç†æœåŠ¡æ— å“åº”");
        
        (resp.policy, resp.value)
    }
}

// ================ å¹¶è¡Œè‡ªå¯¹å¼ˆå·¥ä½œå™¨ ================

/// æ¸¸æˆç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
struct GameStats {
    steps: usize,
    winner: Option<i32>,  // Some(1)=çº¢èƒœ, Some(-1)=é»‘èƒœ, None/Some(0)=å¹³å±€
}

/// å•å±€æ¸¸æˆçš„å®Œæ•´æ•°æ®ï¼ˆåŒ…å«æ ·æœ¬å’Œå…ƒæ•°æ®ï¼‰
#[derive(Debug, Clone)]
struct GameEpisode {
    samples: Vec<(Observation, Vec<f32>, f32, Vec<i32>)>,
    game_length: usize,
    winner: Option<i32>,
}

/// è‡ªå¯¹å¼ˆå·¥ä½œå™¨
pub struct SelfPlayWorker {
    worker_id: usize,
    evaluator: Arc<ChannelEvaluator>,
    mcts_sims: usize,
}

impl SelfPlayWorker {
    pub fn new(
        worker_id: usize,
        evaluator: Arc<ChannelEvaluator>,
        mcts_sims: usize,
    ) -> Self {
        Self {
            worker_id,
            evaluator,
            mcts_sims,
        }
    }

    /// è¿è¡Œä¸€å±€è‡ªå¯¹å¼ˆæ¸¸æˆï¼Œè¿”å›GameEpisode
    pub fn play_episode(&self, episode_num: usize) -> GameEpisode {
        println!("  [Worker-{}] å¼€å§‹ç¬¬ {} å±€æ¸¸æˆ", self.worker_id, episode_num + 1);
        let start_time = Instant::now();
        
        let mut env = DarkChessEnv::new();
        let config = MCTSConfig { num_simulations: self.mcts_sims, cpuct: 1.0 };
        let mut mcts = MCTS::new(&env, self.evaluator.clone(), config);
        
        let mut episode_data = Vec::new();
        let mut step = 0;
        
        // ğŸ› DEBUG: è®°å½•é¦–æ­¥MCTSè¯¦æƒ…
        let debug_first_step = episode_num < 2; // åªè°ƒè¯•å‰2å±€
        
        loop {
            // è¿è¡ŒMCTS
            mcts.run();
            let probs = mcts.get_root_probabilities();
            let masks = env.action_masks();
            
            // ğŸ› DEBUG: æ‰“å°MCTSæ ¹èŠ‚ç‚¹è¯¦æƒ…
            if debug_first_step && step < 3 {
                println!("    [Worker-{}] Step {}: MCTSæ ¹èŠ‚ç‚¹è¯¦æƒ…", self.worker_id, step);
                let top_actions = get_top_k_actions(&probs, 5);
                for (action, prob) in top_actions {
                    println!("      action={}, prob={:.3}", action, prob);
                }
            }
            
            // ä¿å­˜æ•°æ®
            episode_data.push((
                env.get_state(),
                probs.clone(),
                env.get_current_player(),
                masks,
            ));
            
            // é€‰æ‹©åŠ¨ä½œ(ä½¿ç”¨æ›´é•¿çš„é«˜æ¸©æ¢ç´¢æœŸ,å¹¶æé«˜æ¢ç´¢æ¸©åº¦)
            // æ¸¸æˆå¹³å‡æ­¥æ•°åœ¨13æ­¥å·¦å³
            let temperature = if step < 2 { 1.5 } else if step < 10 { 1.2 } else { 0.9 };
            let action = sample_action(&probs, &env, temperature);
            
            // ğŸ› DEBUG: è®°å½•åŠ¨ä½œé€‰æ‹©
            if debug_first_step && step < 3 {
                println!("      é€‰æ‹©: action={}, temp={:.1}", action, temperature);
            }
            
            // æ‰§è¡ŒåŠ¨ä½œ
            match env.step(action, None) {
                Ok((_, _, terminated, truncated, winner)) => {
                    mcts.step_next(&env, action);
                    
                    if terminated || truncated {
                        // åˆ†é…å¥–åŠ±
                        let reward_red = match winner {
                            Some(1) => 1.0,
                            Some(-1) => -1.0,
                            _ => 0.0,
                        };
                        
                        let elapsed = start_time.elapsed();
                        println!("  [Worker-{}] ç¬¬ {} å±€ç»“æŸ: {} æ­¥, èƒœè€…={:?}, è€—æ—¶ {:.1}s", 
                            self.worker_id, episode_num + 1, step, winner, elapsed.as_secs_f64());
                        
                        // ğŸ› DEBUG: æ£€æŸ¥ä»·å€¼æ ‡ç­¾åˆ†å¸ƒ
                        if debug_first_step {
                            let mut red_values = Vec::new();
                            let mut black_values = Vec::new();
                            for (_, _, player, _) in &episode_data {
                                let val = if player.val() == 1 { reward_red } else { -reward_red };
                                if player.val() == 1 {
                                    red_values.push(val);
                                } else {
                                    black_values.push(val);
                                }
                            }
                            println!("    [Worker-{}] ä»·å€¼æ ‡ç­¾ç»Ÿè®¡: çº¢æ–¹æ ·æœ¬æ•°={}, é»‘æ–¹æ ·æœ¬æ•°={}", 
                                self.worker_id, red_values.len(), black_values.len());
                            if !red_values.is_empty() {
                                println!("      çº¢æ–¹ä»·å€¼æ ‡ç­¾: {:.2} (winner={:?})", red_values[0], winner);
                            }
                            if !black_values.is_empty() {
                                println!("      é»‘æ–¹ä»·å€¼æ ‡ç­¾: {:.2} (winner={:?})", black_values[0], winner);
                            }
                        }
                        
                        // å›å¡«ä»·å€¼
                        let mut samples = Vec::new();
                        for (obs, p, player, mask) in episode_data {
                            let val = if player.val() == 1 { reward_red } else { -reward_red };
                            samples.push((obs, p, val, mask));
                        }
                        
                        return GameEpisode {
                            samples,
                            game_length: step,
                            winner,
                        };
                    }
                },
                Err(e) => {
                    eprintln!("[Worker-{}] æ¸¸æˆé”™è¯¯: {}", self.worker_id, e);
                    return GameEpisode {
                        samples: Vec::new(),
                        game_length: step,
                        winner: None,
                    };
                }
            }
            
            step += 1;
            if step > 200 {
                // è¶…è¿‡æœ€å¤§æ­¥æ•°ï¼Œæ¸¸æˆå¹³å±€
                println!("  [Worker-{}] ç¬¬ {} å±€è¶…æ—¶: {} æ­¥", self.worker_id, episode_num + 1, step);
                let mut samples = Vec::new();
                for (obs, p, _, mask) in episode_data {
                    samples.push((obs, p, 0.0, mask));
                }
                return GameEpisode {
                    samples,
                    game_length: step,
                    winner: None,
                };
            }
        }
    }
}

/// åŠ¨ä½œé‡‡æ ·ï¼ˆå¸¦æ¸©åº¦å‚æ•°ï¼‰
fn sample_action(probs: &[f32], env: &DarkChessEnv, temperature: f32) -> usize {
    use rand::distributions::WeightedIndex;
    use rand::prelude::*;
    
    let non_zero_sum: f32 = probs.iter().sum();
    
    if non_zero_sum == 0.0 {
        // å›é€€ï¼šä»æœ‰æ•ˆåŠ¨ä½œä¸­å‡åŒ€é€‰æ‹©
        let masks = env.action_masks();
        let valid_actions: Vec<usize> = masks.iter()
            .enumerate()
            .filter_map(|(i, &m)| if m == 1 { Some(i) } else { None })
            .collect();
        
        let mut rng = thread_rng();
        *valid_actions.choose(&mut rng).expect("æ— æœ‰æ•ˆåŠ¨ä½œ")
    } else {
        // åº”ç”¨æ¸©åº¦å‚æ•°
        let adjusted_probs: Vec<f32> = if temperature != 1.0 {
            let sum: f32 = probs.iter()
                .map(|&p| p.powf(1.0 / temperature))
                .sum();
            probs.iter()
                .map(|&p| p.powf(1.0 / temperature) / sum)
                .collect()
        } else {
            probs.to_vec()
        };
        
        let dist = WeightedIndex::new(&adjusted_probs).unwrap();
        let mut rng = thread_rng();
        dist.sample(&mut rng)
    }
}

/// ğŸ› DEBUG: è·å–top-kåŠ¨ä½œ
fn get_top_k_actions(probs: &[f32], k: usize) -> Vec<(usize, f32)> {
    let mut indexed: Vec<(usize, f32)> = probs.iter()
        .enumerate()
        .map(|(i, &p)| (i, p))
        .collect();
    indexed.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    indexed.into_iter().take(k).collect()
}

// ================ æ•°æ®åº“æ“ä½œï¼ˆå¤ç”¨åŸæœ‰ä»£ç ï¼‰ ================

fn init_database(db_path: &str) -> Result<Connection> {
    let conn = Connection::open(db_path)?;
    
    conn.execute(
        "CREATE TABLE IF NOT EXISTS training_samples (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            iteration INTEGER NOT NULL,
            episode_type TEXT NOT NULL,
            board_state BLOB NOT NULL,
            scalar_state BLOB NOT NULL,
            policy_probs BLOB NOT NULL,
            value_target REAL NOT NULL,
            action_mask BLOB NOT NULL,
            game_length INTEGER NOT NULL,
            step_in_game INTEGER NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )",
        [],
    )?;
    
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_iteration ON training_samples(iteration)",
        [],
    )?;
    
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_episode_type ON training_samples(episode_type)",
        [],
    )?;
    
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_game_length ON training_samples(game_length)",
        [],
    )?;
    
    println!("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {}", db_path);
    Ok(conn)
}

fn save_samples_to_db(
    conn: &mut Connection,
    iteration: usize,
    episode_type: &str,
    samples: &[(Observation, Vec<f32>, f32, Vec<i32>)],
    game_length: usize,
) -> Result<()> {
    let tx = conn.transaction()?;
    {
        let mut stmt = tx.prepare(
            "INSERT INTO training_samples 
             (iteration, episode_type, board_state, scalar_state, policy_probs, value_target, action_mask, game_length, step_in_game) 
             VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9)"
        )?;
        
        for (step_idx, (obs, probs, value, mask)) in samples.iter().enumerate() {
            let board_bytes: Vec<u8> = obs.board.as_slice().unwrap()
                .iter()
                .flat_map(|&x| x.to_le_bytes())
                .collect();
            
            let scalar_bytes: Vec<u8> = obs.scalars.as_slice().unwrap()
                .iter()
                .flat_map(|&x| x.to_le_bytes())
                .collect();
            
            let probs_bytes: Vec<u8> = probs.iter()
                .flat_map(|&x| x.to_le_bytes())
                .collect();
            
            let mask_bytes: Vec<u8> = mask.iter()
                .flat_map(|&x| x.to_le_bytes())
                .collect();
            
            stmt.execute(params![
                iteration as i64,
                episode_type,
                board_bytes,
                scalar_bytes,
                probs_bytes,
                value,
                mask_bytes,
                game_length as i64,
                step_idx as i64,
            ])?;
        }
    }
    tx.commit()?;
    Ok(())
}

fn load_samples_from_db(conn: &Connection) -> Result<Vec<(Observation, Vec<f32>, f32, Vec<i32>)>> {
    let mut stmt = conn.prepare(
        "SELECT board_state, scalar_state, policy_probs, value_target, action_mask 
         FROM training_samples"
    )?;
    
    let samples = stmt.query_map([], |row| {
        let board_bytes: Vec<u8> = row.get(0)?;
        let scalar_bytes: Vec<u8> = row.get(1)?;
        let probs_bytes: Vec<u8> = row.get(2)?;
        let value: f32 = row.get(3)?;
        let mask_bytes: Vec<u8> = row.get(4)?;
        
        let board_data: Vec<f32> = board_bytes.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        let scalar_data: Vec<f32> = scalar_bytes.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        let probs: Vec<f32> = probs_bytes.chunks_exact(4)
            .map(|chunk| f32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        let mask: Vec<i32> = mask_bytes.chunks_exact(4)
            .map(|chunk| i32::from_le_bytes([chunk[0], chunk[1], chunk[2], chunk[3]]))
            .collect();
        
        use ndarray::Array;
        let board = Array::from_shape_vec((2, 8, 3, 4), board_data)
            .expect("Failed to reshape board data");
        let scalars = Array::from_vec(scalar_data);
        
        let obs = Observation { board, scalars };
        
        Ok((obs, probs, value, mask))
    })?;
    
    let mut result = Vec::new();
    for sample in samples {
        result.push(sample?);
    }
    
    Ok(result)
}

// ================ è®­ç»ƒæ­¥éª¤ï¼ˆå¤ç”¨åŸæœ‰ä»£ç ï¼‰ ================

fn train_step(
    opt: &mut nn::Optimizer,
    net: &BanqiNet,
    examples: &[(Observation, Vec<f32>, f32, Vec<i32>)],
    batch_size: usize,
    device: Device,
    epoch: usize,
) -> (f64, f64, f64) {
    if examples.is_empty() { return (0.0, 0.0, 0.0); }
    
    use rand::seq::SliceRandom;
    use rand::thread_rng;
    
    let mut shuffled_examples = examples.to_vec();
    shuffled_examples.shuffle(&mut thread_rng());
    
    let mut total_loss_sum = 0.0;
    let mut policy_loss_sum = 0.0;
    let mut value_loss_sum = 0.0;
    let mut num_samples = 0;
    
    // åŠ¨æ€è°ƒæ•´ç­–ç•¥æƒé‡: æ—©æœŸæ›´æ³¨é‡ç­–ç•¥å­¦ä¹ ,åæœŸå¹³è¡¡
    let policy_weight = 1.5 + (epoch as f32 * 0.1).min(1.0); // ä»1.5é€æ¸å¢åŠ åˆ°2.5
    let value_weight = 2.0; // å¤§å¹…æé«˜ä»·å€¼æƒé‡ (åŸæ¥æ˜¯0.5-1.0éšå¼æƒé‡)
    
    // ğŸ› DEBUG: æ£€æŸ¥æ ·æœ¬ç»Ÿè®¡
    let mut value_stats = Vec::new();
    let mut entropy_stats = Vec::new();
    
    for batch_start in (0..shuffled_examples.len()).step_by(batch_size) {
        let batch_end = (batch_start + batch_size).min(shuffled_examples.len());
        let batch = &shuffled_examples[batch_start..batch_end];
        
        for (obs, target_probs, target_val, masks) in batch.iter() {
            // ğŸ› DEBUG: æ”¶é›†ç»Ÿè®¡æ•°æ®
            value_stats.push(*target_val);
            let entropy: f32 = target_probs.iter()
                .filter(|&&p| p > 1e-8)
                .map(|&p| -p * p.ln())
                .sum();
            entropy_stats.push(entropy);
            
            let board_tensor = Tensor::from_slice(obs.board.as_slice().unwrap()).view([1, 8, 3, 4]).to(device);
            let scalar_tensor = Tensor::from_slice(obs.scalars.as_slice().unwrap()).view([1, 56]).to(device);
            let target_p = Tensor::from_slice(target_probs).view([1, 46]).to(device);
            let target_v = Tensor::from_slice(&[*target_val]).view([1, 1]).to(device);
            
            let mask_vec: Vec<f32> = masks.iter().map(|&m| m as f32).collect();
            let mask_tensor = Tensor::from_slice(&mask_vec).view([1, 46]).to(device);
            
            let (logits, value) = net.forward(&board_tensor, &scalar_tensor);
            
            let masked_logits = &logits + (&mask_tensor - 1.0) * 1e9;
            let log_probs = masked_logits.log_softmax(-1, Kind::Float);
            
            // ç­–ç•¥æŸå¤±: äº¤å‰ç†µ
            let p_loss = (&target_p * &log_probs).sum(Kind::Float).neg() * (policy_weight as f64);
            // ä»·å€¼æŸå¤±: MSE,åŠ å¤§æƒé‡
            let v_loss = value.mse_loss(&target_v, tch::Reduction::Mean) * (value_weight as f64);
            
            let total_loss = &p_loss + &v_loss;
            
            opt.backward_step(&total_loss);
            
            total_loss_sum += total_loss.double_value(&[]);
            policy_loss_sum += p_loss.double_value(&[]) / policy_weight as f64;
            value_loss_sum += v_loss.double_value(&[]) / value_weight as f64;
            num_samples += 1;
        }
    }
    
    // ğŸ› DEBUG: è¾“å‡ºæ ·æœ¬è´¨é‡ç»Ÿè®¡
    if epoch == 0 && !value_stats.is_empty() {
        let avg_value: f32 = value_stats.iter().sum::<f32>() / value_stats.len() as f32;
        let std_value: f32 = (value_stats.iter().map(|v| (v - avg_value).powi(2)).sum::<f32>() / value_stats.len() as f32).sqrt();
        let avg_entropy: f32 = entropy_stats.iter().sum::<f32>() / entropy_stats.len() as f32;
        
        let positive_values = value_stats.iter().filter(|&&v| v > 0.0).count();
        let negative_values = value_stats.iter().filter(|&&v| v < 0.0).count();
        let zero_values = value_stats.iter().filter(|&&v| v == 0.0).count();
        
        println!("    ğŸ› æ ·æœ¬ç»Ÿè®¡: æ€»æ•°={}, ä»·å€¼[avg={:.3}, std={:.3}], ç†µ[avg={:.3}]", 
            value_stats.len(), avg_value, std_value, avg_entropy);
        println!("    ğŸ› ä»·å€¼åˆ†å¸ƒ: æ­£={} ({:.1}%), é›¶={} ({:.1}%), è´Ÿ={} ({:.1}%)",
            positive_values, positive_values as f32 / value_stats.len() as f32 * 100.0,
            zero_values, zero_values as f32 / value_stats.len() as f32 * 100.0,
            negative_values, negative_values as f32 / value_stats.len() as f32 * 100.0);
    }
    
    if num_samples > 0 { 
        (total_loss_sum / num_samples as f64,
         policy_loss_sum / num_samples as f64,
         value_loss_sum / num_samples as f64)
    } else { 
        (0.0, 0.0, 0.0)
    }
}

// ================ ä¸»è®­ç»ƒå¾ªç¯ ================

/// åœºæ™¯éªŒè¯ç»“æœ
#[derive(Debug, Clone)]
struct ScenarioResult {
    value: f32,
    unmasked_probs: Vec<f32>,  // åŸå§‹softmaxæ¦‚ç‡
    masked_probs: Vec<f32>,    // åº”ç”¨maskåçš„æ¦‚ç‡
}

/// éªŒè¯æ¨¡å‹åœ¨æ ‡å‡†åœºæ™¯ä¸Šçš„è¡¨ç°ï¼Œè¿”å›è¯¦ç»†æ•°æ®
fn validate_model_on_scenarios(vs: &nn::VarStore, device: Device, _iteration: usize) -> (ScenarioResult, ScenarioResult) {
    use banqi_3x4::game_env::Player;
    
    let net = BanqiNet::new(&vs.root());
    
    // åœºæ™¯1: R_A vs B_A
    let scenario1_result = {
        let mut env = DarkChessEnv::new();
        env.setup_two_advisors(Player::Black);
        
        let obs = env.get_state();
        let board_tensor = Tensor::from_slice(obs.board.as_slice().unwrap())
            .view([1, 8, 3, 4])
            .to(device);
        let scalar_tensor = Tensor::from_slice(obs.scalars.as_slice().unwrap())
            .view([1, 56])
            .to(device);
        
        let masks: Vec<f32> = env.action_masks().iter().map(|&m| m as f32).collect();
        let mask_tensor = Tensor::from_slice(&masks).to(device).view([1, 46]);
        
        let (logits, value) = tch::no_grad(|| net.forward(&board_tensor, &scalar_tensor));
        
        // ğŸ› DEBUG: æ‰“å°åŸå§‹logits
        let logits_vec: Vec<f32> = (0..46).map(|i| logits.double_value(&[0, i]) as f32).collect();
        let top_logits = get_top_k_actions(&logits_vec, 5);
        println!("      ğŸ› åŸå§‹logits (top-5): {:?}", top_logits);
        
        // æœªåº”ç”¨maskçš„æ¦‚ç‡åˆ†å¸ƒ
        let unmasked_probs_tensor = logits.softmax(-1, Kind::Float);
        let unmasked_probs: Vec<f32> = (0..46).map(|i| unmasked_probs_tensor.double_value(&[0, i]) as f32).collect();
        
        // åº”ç”¨maskåçš„æ¦‚ç‡åˆ†å¸ƒ
        let masked_logits = &logits + (&mask_tensor - 1.0) * 1e9;
        let masked_probs_tensor = masked_logits.softmax(-1, Kind::Float);
        let masked_probs: Vec<f32> = (0..46).map(|i| masked_probs_tensor.double_value(&[0, i]) as f32).collect();
        
        let value_pred: f32 = value.squeeze().double_value(&[]) as f32;
        
        // ğŸ› DEBUG: æ£€æŸ¥æœ‰æ•ˆåŠ¨ä½œ
        let valid_actions: Vec<usize> = masks.iter()
            .enumerate()
            .filter_map(|(i, &m)| if m == 1.0 { Some(i) } else { None })
            .collect();
        println!("      ğŸ› æœ‰æ•ˆåŠ¨ä½œæ•°: {}, åŒ…æ‹¬: {:?}", valid_actions.len(), &valid_actions[..valid_actions.len().min(10)]);
        
        println!("    åœºæ™¯1 (R_A vs B_A): value={:.3}", value_pred);
        println!("      æœªåº”ç”¨mask: a38={:.1}%, a39={:.1}%, a40={:.1}%", 
            unmasked_probs[38]*100.0, unmasked_probs[39]*100.0, unmasked_probs[40]*100.0);
        println!("      åº”ç”¨maskå: a38={:.1}%, a39={:.1}%, a40={:.1}%", 
            masked_probs[38]*100.0, masked_probs[39]*100.0, masked_probs[40]*100.0);
        println!("      æœŸæœ›: action38ä¸»å¯¼(>90%), valueåº”åå‘å½“å‰ç©å®¶(é»‘æ–¹)ç•¥ä¼˜æˆ–å¹³å±€");
        
        ScenarioResult {
            value: value_pred,
            unmasked_probs,
            masked_probs,
        }
    };
    
    // åœºæ™¯2: Hidden Threat
    let scenario2_result = {
        let mut env = DarkChessEnv::new();
        env.setup_hidden_threats();
        
        let obs = env.get_state();
        let board_tensor = Tensor::from_slice(obs.board.as_slice().unwrap())
            .view([1, 8, 3, 4])
            .to(device);
        let scalar_tensor = Tensor::from_slice(obs.scalars.as_slice().unwrap())
            .view([1, 56])
            .to(device);
        
        let masks: Vec<f32> = env.action_masks().iter().map(|&m| m as f32).collect();
        let mask_tensor = Tensor::from_slice(&masks).to(device).view([1, 46]);
        
        let (logits, value) = tch::no_grad(|| net.forward(&board_tensor, &scalar_tensor));
        
        // ğŸ› DEBUG: æ‰“å°åŸå§‹logits
        let logits_vec: Vec<f32> = (0..46).map(|i| logits.double_value(&[0, i]) as f32).collect();
        let top_logits = get_top_k_actions(&logits_vec, 5);
        println!("      ğŸ› åŸå§‹logits (top-5): {:?}", top_logits);
        
        // æœªåº”ç”¨maskçš„æ¦‚ç‡åˆ†å¸ƒ
        let unmasked_probs_tensor = logits.softmax(-1, Kind::Float);
        let unmasked_probs: Vec<f32> = (0..46).map(|i| unmasked_probs_tensor.double_value(&[0, i]) as f32).collect();
        
        // åº”ç”¨maskåçš„æ¦‚ç‡åˆ†å¸ƒ
        let masked_logits = &logits + (&mask_tensor - 1.0) * 1e9;
        let masked_probs_tensor = masked_logits.softmax(-1, Kind::Float);
        let masked_probs: Vec<f32> = (0..46).map(|i| masked_probs_tensor.double_value(&[0, i]) as f32).collect();
        
        let value_pred: f32 = value.squeeze().double_value(&[]) as f32;
        
        // ğŸ› DEBUG: æ£€æŸ¥æœ‰æ•ˆåŠ¨ä½œ
        let valid_actions: Vec<usize> = masks.iter()
            .enumerate()
            .filter_map(|(i, &m)| if m == 1.0 { Some(i) } else { None })
            .collect();
        println!("      ğŸ› æœ‰æ•ˆåŠ¨ä½œæ•°: {}, åŒ…æ‹¬: {:?}", valid_actions.len(), &valid_actions[..valid_actions.len().min(10)]);
        
        println!("    åœºæ™¯2 (Hidden Threat): value={:.3}", value_pred);
        println!("      æœªåº”ç”¨mask: a3={:.1}%, a5={:.1}%", 
            unmasked_probs[3]*100.0, unmasked_probs[5]*100.0);
        println!("      åº”ç”¨maskå: a3={:.1}%, a5={:.1}%", 
            masked_probs[3]*100.0, masked_probs[5]*100.0);
        println!("      æœŸæœ›: action3ä¸»å¯¼(>90%), valueåº”èƒ½åæ˜ ä½ç½®ä¼˜åŠ¿");
        
        ScenarioResult {
            value: value_pred,
            unmasked_probs,
            masked_probs,
        }
    };
    
    (scenario1_result, scenario2_result)
}

pub fn parallel_train_loop() -> Result<()> {
    // è®¾å¤‡é…ç½®
    let cuda_available = tch::Cuda::is_available();
    println!("CUDA available: {}", cuda_available);
    
    let device = if cuda_available {
        println!("Using CUDA device 0");
        Device::Cuda(0)
    } else {
        println!("Using CPU");
        Device::Cpu
    };
    
    // å¹¶è¡Œé…ç½®
    let num_workers = (num_cpus::get() * 2).max(8); // å·¥ä½œçº¿ç¨‹æ•°:CPUæ ¸å¿ƒæ•°çš„2å€,è‡³å°‘8ä¸ª
    let mcts_sims = 1200; // è¿›ä¸€æ­¥æé«˜MCTSè´¨é‡ - è¿™æ˜¯è®­ç»ƒæ•°æ®è´¨é‡çš„å…³é”®
    let num_episodes_per_iteration = 80; // å¢åŠ æ¸¸æˆæ•°ä»¥æ”¶é›†æ›´å¤šæ ·æœ¬
    let inference_batch_size = 64.min(num_workers); // æ¨ç†æ‰¹é‡å¤§å°
    let inference_timeout_ms = 5; // æ‰¹é‡æ¨ç†è¶…æ—¶(æ¯«ç§’)- è¿›ä¸€æ­¥é™ä½ä»¥æé«˜å“åº”é€Ÿåº¦
    let max_buffer_size = 25000; // ç»éªŒå›æ”¾ç¼“å†²åŒº - ä¿ç•™æœ€è¿‘25000ä¸ªæ ·æœ¬
    
    println!("\n=== å¹¶è¡Œè®­ç»ƒé…ç½® ===");
    println!("å·¥ä½œçº¿ç¨‹æ•°: {}", num_workers);
    println!("æ¯è½®æ¸¸æˆæ•°: {}", num_episodes_per_iteration);
    println!("MCTSæ¨¡æ‹Ÿæ¬¡æ•°: {}", mcts_sims);
    println!("æ¨ç†æ‰¹é‡å¤§å°: {}", inference_batch_size);
    println!("æ¨ç†è¶…æ—¶: {}ms", inference_timeout_ms);
    println!("ç»éªŒå›æ”¾ç¼“å†²åŒº: {}", max_buffer_size);
    
    // åˆå§‹åŒ–æ•°æ®åº“
    let db_path = "training_samples.db";
    let mut conn = init_database(db_path)?;
    
    // åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    let vs = nn::VarStore::new(device);
    // **é‡è¦**: ç«‹å³åˆ›å»ºç½‘ç»œä»¥åˆå§‹åŒ–æ‰€æœ‰å‚æ•°
    let _init_net = BanqiNet::new(&vs.root());
    
    let learning_rate = 2e-4; // é™ä½å­¦ä¹ ç‡é¿å…éœ‡è¡ (ä»5e-4é™åˆ°2e-4)
    let mut opt = nn::Adam::default().build(&vs, learning_rate)?;
    
    // è®­ç»ƒè¶…å‚æ•°
    let num_iterations = 200;
    let batch_size = 128; // å¢å¤§æ‰¹é‡ä»¥ç¨³å®šè®­ç»ƒ
    let epochs_per_iteration = 5; // å¤§å¹…å‡å°‘epoché¿å…è¿‡æ‹Ÿåˆ (ä»15é™åˆ°5)
    
    // ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½å·²æœ‰æ•°æ®è®­ç»ƒ
    println!("\n=== ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½å·²æœ‰æ•°æ® ===");
    let existing_samples = load_samples_from_db(&conn)?;
    if !existing_samples.is_empty() {
        println!("åŠ è½½äº† {} ä¸ªæ ·æœ¬", existing_samples.len());
        
        // åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç½‘ç»œç”¨äºåˆå§‹è®­ç»ƒ
        let temp_net = BanqiNet::new(&vs.root());
        
        for epoch in 0..5 {
            let (loss, p_loss, v_loss) = train_step(&mut opt, &temp_net, &existing_samples, batch_size, device, epoch);

                println!("  Epoch {}/5, Loss={:.4} (Policy={:.4}, Value={:.4})", 
                    epoch + 1, loss, p_loss, v_loss);
            
        }
        
        vs.save("banqi_model_pretrained.ot")?;
        println!("å·²ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹");
    }
    
    // ç¬¬äºŒé˜¶æ®µï¼šå¹¶è¡Œè‡ªå¯¹å¼ˆè®­ç»ƒ
    println!("\n=== ç¬¬äºŒé˜¶æ®µï¼šå¹¶è¡Œè‡ªå¯¹å¼ˆè®­ç»ƒ ===");
    
    // åˆå§‹åŒ–CSVæ—¥å¿—
    let csv_path = "training_log.csv";
    TrainingLog::write_header(csv_path)?;
    println!("CSVæ—¥å¿—æ–‡ä»¶: {}", csv_path);
    
    // ç»éªŒå›æ”¾ç¼“å†²åŒº
    let mut replay_buffer: Vec<(Observation, Vec<f32>, f32, Vec<i32>)> = Vec::new();
    
    for iteration in 0..num_iterations {
        println!("\n========== Iteration {}/{} ==========", iteration, num_iterations);
        
        // åˆ›å»ºæ¨ç†é€šé“
        let (req_tx, req_rx) = mpsc::channel::<InferenceRequest>();
        
        // å¯åŠ¨æ¨ç†æœåŠ¡å™¨çº¿ç¨‹ - åœ¨çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„ç½‘ç»œæ¥é¿å…æ‰€æœ‰æƒé—®é¢˜
        let temp_model_path = format!("banqi_model_iter_{}_temp.ot", iteration);
        vs.save(&temp_model_path)?;
        let temp_model_path_clone = temp_model_path.clone();
        
        let inference_handle = thread::spawn(move || {
            match InferenceServer::new(
                &temp_model_path_clone,
                device,
                req_rx,
                inference_batch_size,
                inference_timeout_ms,
            ) {
                Ok(server) => server.run(),
                Err(e) => {
                    eprintln!("[InferenceServer] åˆå§‹åŒ–å¤±è´¥: {}", e);
                }
            }
        });
        
        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        let mut worker_handles = Vec::new();
        let mut result_rxs = Vec::new();
        
        for worker_id in 0..num_workers {
            let req_tx_clone = req_tx.clone();
            let (result_tx, result_rx) = mpsc::channel();
            result_rxs.push(result_rx);
            
            let handle = thread::spawn(move || {
                let evaluator = Arc::new(ChannelEvaluator::new(req_tx_clone));
                let worker = SelfPlayWorker::new(worker_id, evaluator, mcts_sims);
                
                let mut all_episodes = Vec::new();
                let episodes_per_worker = (num_episodes_per_iteration + num_workers - 1) / num_workers;
                
                for ep in 0..episodes_per_worker {
                    let episode = worker.play_episode(ep);
                    all_episodes.push(episode);
                }
                
                println!("  [Worker-{}] å®Œæˆæ‰€æœ‰ {} å±€æ¸¸æˆ", worker_id, episodes_per_worker);
                result_tx.send(all_episodes).expect("æ— æ³•å‘é€ç»“æœ");
            });
            
            worker_handles.push(handle);
        }
        
        // å…³é—­ä¸»è¯·æ±‚å‘é€ç«¯ï¼Œä»¥ä¾¿æ¨ç†æœåŠ¡å™¨çŸ¥é“ä½•æ—¶é€€å‡º
        drop(req_tx);
        
        // æ”¶é›†æ‰€æœ‰å·¥ä½œçº¿ç¨‹çš„ç»“æœ
        let mut all_episodes = Vec::new();
        for result_rx in result_rxs {
            if let Ok(episodes) = result_rx.recv() {
                all_episodes.extend(episodes);
            }
        }
        
        // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
        for handle in worker_handles {
            handle.join().expect("å·¥ä½œçº¿ç¨‹å¼‚å¸¸");
        }
        
        // ç­‰å¾…æ¨ç†æœåŠ¡å™¨é€€å‡º
        inference_handle.join().expect("æ¨ç†æœåŠ¡å™¨å¼‚å¸¸");
        
        // æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶
        let _ = std::fs::remove_file(&temp_model_path);
        
        // ä»episodesä¸­æå–ç»Ÿè®¡ä¿¡æ¯å’Œæ ·æœ¬
        let mut all_samples = Vec::new();
        let mut all_game_stats = Vec::new();
        for episode in &all_episodes {
            all_samples.extend(episode.samples.clone());
            all_game_stats.push(GameStats {
                steps: episode.game_length,
                winner: episode.winner,
            });
        }
        
        println!("  æ”¶é›†äº† {} ä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆæ¥è‡ª {} å±€æ¸¸æˆï¼‰", all_samples.len(), all_episodes.len());
        
        // è®¡ç®—æ¸¸æˆç»Ÿè®¡ä¿¡æ¯
        let total_games = all_game_stats.len();
        let total_steps: usize = all_game_stats.iter().map(|s| s.steps).sum();
        let avg_game_steps = if total_games > 0 { total_steps as f32 / total_games as f32 } else { 0.0 };
        
        let mut red_wins = 0;
        let mut black_wins = 0;
        let mut draws = 0;
        for stat in &all_game_stats {
            match stat.winner {
                Some(1) => red_wins += 1,
                Some(-1) => black_wins += 1,
                _ => draws += 1,
            }
        }
        
        let red_win_ratio = if total_games > 0 { red_wins as f32 / total_games as f32 } else { 0.0 };
        let black_win_ratio = if total_games > 0 { black_wins as f32 / total_games as f32 } else { 0.0 };
        let draw_ratio = if total_games > 0 { draws as f32 / total_games as f32 } else { 0.0 };
        
        // è®¡ç®—ç­–ç•¥ç†µå’Œé«˜ç½®ä¿¡åº¦æ ·æœ¬æ¯”ä¾‹
        let mut total_entropy = 0.0f32;
        let mut high_confidence_count = 0;
        
        // ğŸ› DEBUG: æ”¶é›†ç­–ç•¥åˆ†å¸ƒç»Ÿè®¡
        let mut max_probs = Vec::new();
        let mut action_diversity = Vec::new();
        
        for (_, probs, _, _) in &all_samples {
            let entropy: f32 = probs.iter()
                .filter(|&&p| p > 1e-8)
                .map(|&p| -p * p.ln())
                .sum();
            total_entropy += entropy;
            if entropy < 1.5 {
                high_confidence_count += 1;
            }
            
            // ğŸ› ç»Ÿè®¡æœ€å¤§æ¦‚ç‡å’Œæœ‰æ•ˆåŠ¨ä½œæ•°
            let max_prob = probs.iter().cloned().fold(0.0f32, f32::max);
            max_probs.push(max_prob);
            let num_significant_actions = probs.iter().filter(|&&p| p > 0.01).count();
            action_diversity.push(num_significant_actions);
        }
        
        let avg_policy_entropy = if !all_samples.is_empty() { 
            total_entropy / all_samples.len() as f32 
        } else { 
            0.0 
        };
        let high_confidence_ratio = if !all_samples.is_empty() {
            high_confidence_count as f32 / all_samples.len() as f32
        } else {
            0.0
        };
        
        // æ•°æ®è´¨é‡è¯Šæ–­
        if iteration % 10 == 0 {
            println!("  ========== æ•°æ®è´¨é‡è¯Šæ–­ ==========");
            println!("    æ¸¸æˆç»Ÿè®¡: æ€»å±€æ•°={}, å¹³å‡æ­¥æ•°={:.1}", total_games, avg_game_steps);
            println!("    æ¸¸æˆç»“æœ: çº¢èƒœ={} ({:.1}%), å¹³å±€={} ({:.1}%), é»‘èƒœ={} ({:.1}%)", 
                red_wins, red_win_ratio * 100.0,
                draws, draw_ratio * 100.0,
                black_wins, black_win_ratio * 100.0);
            println!("    ç­–ç•¥è´¨é‡: å¹³å‡ç†µ={:.3}, é«˜ç½®ä¿¡åº¦æ ·æœ¬={} ({:.1}%)", 
                avg_policy_entropy, high_confidence_count, high_confidence_ratio * 100.0);
            
            // ğŸ› DEBUG: è¾“å‡ºç­–ç•¥åˆ†å¸ƒè´¨é‡
            if !max_probs.is_empty() {
                let avg_max_prob: f32 = max_probs.iter().sum::<f32>() / max_probs.len() as f32;
                let avg_diversity: f32 = action_diversity.iter().map(|&x| x as f32).sum::<f32>() / action_diversity.len() as f32;
                println!("    ğŸ› ç­–ç•¥åˆ†å¸ƒ: å¹³å‡æœ€å¤§æ¦‚ç‡={:.3}, å¹³å‡æœ‰æ•ˆåŠ¨ä½œæ•°={:.1}", avg_max_prob, avg_diversity);
                
                // ç»Ÿè®¡å®Œå…¨å‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬ï¼ˆå¯èƒ½è¡¨ç¤ºMCTSæœªæ”¶æ•›ï¼‰
                let uniform_samples = max_probs.iter().filter(|&&p| p < 0.1).count();
                println!("    ğŸ› å¼‚å¸¸æ ·æœ¬: è¿‘ä¼¼å‡åŒ€åˆ†å¸ƒ={} ({:.1}%)", 
                    uniform_samples, uniform_samples as f32 / max_probs.len() as f32 * 100.0);
            }
        }
        
        // ä¿å­˜æ ·æœ¬åˆ°æ•°æ®åº“ï¼ˆæŒ‰episodeåˆ†åˆ«ä¿å­˜ï¼Œå¸¦æ¸¸æˆé•¿åº¦ä¿¡æ¯ï¼‰
        for episode in &all_episodes {
            save_samples_to_db(&mut conn, iteration, "self_play", &episode.samples, episode.game_length)?;
        }
        
        // ä¿å­˜æ–°æ ·æœ¬æ•°é‡ï¼ˆåœ¨ç§»åŠ¨all_samplesä¹‹å‰ï¼‰
        let new_samples_count = all_samples.len();
        
        // æ›´æ–°ç»éªŒå›æ”¾ç¼“å†²åŒº
        replay_buffer.extend(all_samples);
        if replay_buffer.len() > max_buffer_size {
            // ä¿ç•™æœ€æ–°çš„æ ·æœ¬
            let remove_count = replay_buffer.len() - max_buffer_size;
            replay_buffer.drain(0..remove_count);
        }
        println!("  ç»éªŒå›æ”¾ç¼“å†²åŒº: {} ä¸ªæ ·æœ¬", replay_buffer.len());
        
        println!("  å¼€å§‹è®­ç»ƒ...");
        
        // è·å–å½“å‰è®­ç»ƒepochçš„ç­–ç•¥å’Œä»·å€¼æŸå¤±æƒé‡
        let policy_weight = 1.5 + (0 as f32 * 0.1).min(1.0); // ä»train_stepè·å– - è¿™é‡Œå–ç¬¬ä¸€ä¸ªepochçš„å€¼
        let value_weight = 2.0;
        
        // è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ç»éªŒå›æ”¾ç¼“å†²åŒºè€Œéä»…å½“å‰æ ·æœ¬
        let temp_net = BanqiNet::new(&vs.root());
        let mut total_losses = Vec::new();
        let mut policy_losses = Vec::new();
        let mut value_losses = Vec::new();
        
        let train_start = Instant::now();
        for epoch in 0..epochs_per_iteration {
            let (loss, p_loss, v_loss) = train_step(&mut opt, &temp_net, &replay_buffer, batch_size, device, epoch);
            total_losses.push(loss);
            policy_losses.push(p_loss);
            value_losses.push(v_loss);
            
            if (epoch + 1) % 2 == 0 {
                println!("  Epoch {}/{}, Loss={:.4} (Policy={:.4}, Value={:.4})", 
                    epoch + 1, epochs_per_iteration, loss, p_loss, v_loss);
            }
        }
        
        let train_elapsed = train_start.elapsed();
        let avg_loss: f64 = total_losses.iter().sum::<f64>() / total_losses.len() as f64;
        let avg_p_loss: f64 = policy_losses.iter().sum::<f64>() / policy_losses.len() as f64;
        let avg_v_loss: f64 = value_losses.iter().sum::<f64>() / value_losses.len() as f64;
        println!("  è®­ç»ƒå®Œæˆ,è€—æ—¶ {:.1}s,å¹³å‡Loss: {:.4} (Policy={:.4}, Value={:.4})", 
            train_elapsed.as_secs_f64(), avg_loss, avg_p_loss, avg_v_loss);
        
        // éªŒè¯æ¨¡å‹æ€§èƒ½å¹¶æ”¶é›†åœºæ™¯æ•°æ®
        println!("\n  ========== æ¨¡å‹éªŒè¯ (Iteration {}) ==========", iteration);
        let (scenario1, scenario2) = validate_model_on_scenarios(&vs, device, iteration);
        
        // æ„å»ºè®­ç»ƒæ—¥å¿—
        let log = TrainingLog {
            iteration,
            avg_total_loss: avg_loss,
            avg_policy_loss: avg_p_loss,
            avg_value_loss: avg_v_loss,
            policy_loss_weight: policy_weight as f64,
            value_loss_weight: value_weight as f64,
            
            scenario1_value: scenario1.value,
            scenario1_unmasked_a38: scenario1.unmasked_probs[38],
            scenario1_unmasked_a39: scenario1.unmasked_probs[39],
            scenario1_unmasked_a40: scenario1.unmasked_probs[40],
            scenario1_masked_a38: scenario1.masked_probs[38],
            scenario1_masked_a39: scenario1.masked_probs[39],
            scenario1_masked_a40: scenario1.masked_probs[40],
            
            scenario2_value: scenario2.value,
            scenario2_unmasked_a3: scenario2.unmasked_probs[3],
            scenario2_unmasked_a5: scenario2.unmasked_probs[5],
            scenario2_masked_a3: scenario2.masked_probs[3],
            scenario2_masked_a5: scenario2.masked_probs[5],
            
            new_samples_count,
            replay_buffer_size: replay_buffer.len(),
            avg_game_steps,
            red_win_ratio,
            draw_ratio,
            black_win_ratio,
            avg_policy_entropy,
            high_confidence_ratio,
        };
        
        // å†™å…¥CSV
        if let Err(e) = log.append_to_csv(csv_path) {
            eprintln!("  è­¦å‘Š: æ— æ³•å†™å…¥CSVæ—¥å¿—: {}", e);
        } else {
            println!("  å·²å†™å…¥è®­ç»ƒæ—¥å¿—åˆ° {}", csv_path);
        }
        
        // ä¿å­˜æ¨¡å‹
        vs.save(format!("banqi_model_{}.ot", iteration))?;
        if iteration == num_iterations - 1 {
            vs.save("banqi_model_latest.ot")?;
        }
    }
    
    println!("\nè®­ç»ƒå®Œæˆï¼");
    Ok(())
}

fn main() {
    if let Err(e) = parallel_train_loop() {
        eprintln!("è®­ç»ƒå¤±è´¥: {}", e);
    }
}
